{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Sentimento\n",
    "\n",
    "### Vinícius Nascimento Silva - 7557626\n",
    "\n",
    "#### Bibliotecas utilizadas\n",
    "\n",
    "Neste trabalho foi utilizada a biblioteca *NLTK* para o processamento do texto e a biblioteca *Sklearn* como fornecedora de algorítmos de aprendizado.\n",
    "\n",
    "Outras bibliotecas utilizadas foram a *Panda* para a manipulação de arquivos csv e a numpy para a manipulação de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pyprind\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento dos Tweets\n",
    "\n",
    "A estratégia utilizada para criar a *Bag of Words* foi extrair as informações dos tweets foi tentar relacionar palavras com mesma raíz. Para isso foi utilizada técnicas de **stemização** e **lematização**. A lematização usa um dicionário para determinar qual é a raiz da palavra, mas não consegue diferenciar substantivos de verbos e a tokenização necessária para esse preprocessamento é muito lenta, então a stemização ajuda a reduzir as palavras não tratadas pela lematização.\n",
    "\n",
    "Além as raízes das palavras, as pontuações '!' e '?' também foram consideradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 2500 ,tokenizer = None,preprocessor = None,stop_words = None) \n",
    "stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def reduce(words):\n",
    "    words = map(lemmatizer.lemmatize, words)\n",
    "    words = map(stemmer.stem, words)\n",
    "    return words\n",
    "\n",
    "def tweets_to_words( raw_review ):    \n",
    "    \n",
    "    #Remove caracteres indesejados\n",
    "    letters_only = re.sub(\"[^a-zA-Z!?]\", \" \", raw_review) \n",
    "    #Separa 'Camel Case'\n",
    "    separated = re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",letters_only)\n",
    "    #Separa pontuação\n",
    "    separated = re.sub(\"([a-zA-Z])([?!])\",\"\\g<1> \\g<2>\",separated)\n",
    "    separated = re.sub(\"([?!])([?!])\",\"\\g<1> \\g<2>\",separated)\n",
    "    \n",
    "    #Quebra palavras e deixa tudo minúsculo\n",
    "    words = separated.lower().split()\n",
    "    \n",
    "    #Tira palavras sem sgnificados\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    #Reduz palavras para as suas raízes\n",
    "    reduced_words = reduce(meaningful_words)\n",
    "    \n",
    "    return(\" \".join(reduced_words))\n",
    "\n",
    "def clear_tweets(tweets, verbose = False):\n",
    "    n_tweets = tweets.size\n",
    "    clean_tweets = []\n",
    "    if verbose : bar = pyprind.ProgBar(n_tweets)\n",
    "    for i in range( 0, n_tweets ):\n",
    "        clean_tweets.append(tweets_to_words(tweets[i]))\n",
    "        if verbose : bar.update()\n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outras Features\n",
    "\n",
    "A compania aérea e a quantidade de retweets também são features e foram implementadas de forma direta.\n",
    "\n",
    "Uma questão interessante seria se o horário do tweet é um indicativo de sentimento. Para isso avaliamos o horário como a distância para o meio dia, ou seja, caso o tweet tenha sido publicado às 16h, o valor da feature é 4, cas o tenha sido ás 3 da manhã, o valor é 9. A ideia foi manter horários próximos com valores próximos e evitar \"quebras\" arbitrárias.\n",
    "\n",
    "Uma aparente melhora no aprendizado pode ser observada, mas nada conclusivo:\n",
    "\n",
    "Algorítmo | Sem feature de tempo | Centralizado às 12h | Centralizado às 6h\n",
    ":---: | :---: | :---: | :---:\n",
    "Perceptron | 85,43% | 88,52% | 89,05%\n",
    "Linear Regression | 81,38% | 81,43% | 81,43%\n",
    "Logistic Regression | 90,06% | 90,21% | 90,11%\n",
    "Random Forest | 87,55% | 88,23% | 87,94%\n",
    "\n",
    "Talvez, com mais dados, possamos indentificar se há realmente uma influência do horário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizerAirline = CountVectorizer(analyzer = \"word\",tokenizer = None,preprocessor = None,stop_words = None)\n",
    "\n",
    "def clean_airline_name(airlines) :\n",
    "    cleaned_names = []\n",
    "    for airline_name in airlines :\n",
    "        cleaned_names.append(re.sub(\" \", \"\", airline_name))\n",
    "    return cleaned_names\n",
    "\n",
    "def generate_time_features(time_vector) :\n",
    "    distances = []\n",
    "    for string_date in time_vector :\n",
    "        hour_of_the_day = datetime.strptime(string_date, \"%Y-%m-%d %H:%M:%S %z\").hour\n",
    "        distance_from_midday = abs(12 - hour_of_the_day)\n",
    "        distances.append(distance_from_midday)\n",
    "    return np.matrix(distances).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(data, verbose = False, train = False) :\n",
    "    clean_tweets = clear_tweets(data['text'], verbose)\n",
    "    airlines = clean_airline_name(data['airline'])\n",
    "    \n",
    "    if train :\n",
    "        word_features = vectorizer.fit_transform(clean_tweets).toarray()\n",
    "        airlines_features = vectorizerAirline.fit_transform(airlines).toarray()\n",
    "    else :\n",
    "        word_features = vectorizer.transform(clean_tweets).toarray()\n",
    "        airlines_features = vectorizerAirline.transform(airlines).toarray()\n",
    "    \n",
    "    time_features = generate_time_features(data['tweet_created'])\n",
    "    retweets = np.matrix(data[\"retweet_count\"])\n",
    "    \n",
    "    return np.hstack((word_features, retweets.transpose(), airlines_features, time_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorítmos\n",
    "\n",
    "Para decidir que algorítmo será utilizado para enviar os dados para o Kaggle separei 2000 tweets marcados para testar o erro fora da amostra. Os algorítmos escolhidos foram os vistos em aula (com a excessão de redes neurais) e o algorítmo de floresta aleatória utilizado no tutorial do próprio kaggle. O *Logistic Regression* teve a melhor performance na validação com um pouco menos de 10% de erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5999, 2508)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:11\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"treinamento_parte.csv\", header=0,delimiter=\",\")\n",
    "train_data_features = get_features(train, verbose=True, train=True)\n",
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "forest = forest.fit( train_data_features, train[\"airline_sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(n_iter=100)\n",
    "perceptron = perceptron.fit(train_data_features, train[\"airline_sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression = regression.fit(train_data_features, train[\"airline_sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "logistic = logistic.fit(train_data_features, train[\"airline_sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2073, 2508)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:02\n"
     ]
    }
   ],
   "source": [
    "validation = pd.read_csv(\"validacao.csv\", header=0,delimiter=\",\")\n",
    "validation_data_features = get_features(validation, verbose=True, train=False)\n",
    "print(validation_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron:  0.8895320791123975\n",
      "LinearRegression:  0.8678244090689822\n",
      "LogisticRegression:  0.9073806078147613\n",
      "Random Forest:  0.8876025084418717\n"
     ]
    }
   ],
   "source": [
    "perceptron_result = perceptron.predict(validation_data_features)\n",
    "\n",
    "regression_result = regression.predict(validation_data_features)\n",
    "regression_result = list(map(lambda x: x/abs(x),regression_result))\n",
    "\n",
    "forest_result = forest.predict(validation_data_features)\n",
    "\n",
    "logistic_result = logistic.predict(validation_data_features)\n",
    "\n",
    "real_results = validation[\"airline_sentiment\"]\n",
    "\n",
    "\n",
    "def get_performance(test_results) :\n",
    "    performance = 0.0\n",
    "    for test, real in zip(test_results, real_results) :\n",
    "        if test == real :\n",
    "            performance += 1\n",
    "    return performance/real_results.size\n",
    "    \n",
    "\n",
    "print (\"Perceptron: \", get_performance(perceptron_result))\n",
    "print (\"LinearRegression: \", get_performance(regression_result))\n",
    "print (\"LogisticRegression: \", get_performance(logistic_result))\n",
    "print (\"Random Forest: \", get_performance(forest_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle\n",
    "\n",
    "E este foi o código utilizado para fazer a submissão no kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:09\n",
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    }
   ],
   "source": [
    "final_train = pd.read_csv(\"treinamento.csv\", header=0,delimiter=\",\")\n",
    "final_train_features = get_features(final_train, verbose=True, train=True)\n",
    "\n",
    "logistic = logistic.fit(final_train_features, final_train['airline_sentiment'])\n",
    "\n",
    "final_test = pd.read_csv(\"teste-sem-resposta.csv\", header=0,delimiter=\",\")\n",
    "final_test_features = get_features(final_test, verbose=True, train=False)\n",
    "\n",
    "logistic_result = logistic.predict(final_test_features)\n",
    "\n",
    "output = pd.DataFrame(\\\n",
    "    data={\"id\":final_test[\"id\"], \\\n",
    "          \"airline_sentiment\":logistic_result})\n",
    "\n",
    "output.to_csv( \"resultado.csv\", index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
